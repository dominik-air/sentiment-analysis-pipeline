{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6df371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sqlalchemy import MetaData, Table, select, desc, func\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3c582",
   "metadata": {},
   "source": [
    "For one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8840d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        model_name = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess(text):\n",
    "            new_text = []\n",
    "            for t in text.split(\" \"):\n",
    "                t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "                t = 'http' if t.startswith('http') else t\n",
    "                new_text.append(t)\n",
    "            return \" \".join(new_text) \n",
    "    \n",
    "    def display_result(self, scores):\n",
    "        ranking = np.argsort(scores)\n",
    "        ranking = ranking[::-1]\n",
    "        \n",
    "        for i in range(scores.shape[0]):\n",
    "            l = self.config.id2label[ranking[i]]\n",
    "            s = scores[ranking[i]]\n",
    "            print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "            \n",
    "    def insert_result_into_dataframe(self, scores):\n",
    "        score2text = '\\n1;1;' + \";\".join(str(s) for s in scores)\n",
    "        \n",
    "        with open('results.csv','a') as f:\n",
    "            f.write(score2text)\n",
    "\n",
    "    def predict(self, text):\n",
    "        text = Model.preprocess(text)\n",
    "        encoded_input = self.tokenizer(text, return_tensors='pt')\n",
    "        output = self.model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2223cf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "m = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c67076bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72357625, 0.2286794 , 0.04774441], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = m.predict(\"Covid cases are increasing fast!\")\n",
    "# m.insert_result_into_dataframe(m.predict(\"Covid cases are increasing fast!\"))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337eb3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce44838",
   "metadata": {},
   "source": [
    "For dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9cb4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, engine):\n",
    "        model_name = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        \n",
    "        self.engine = engine\n",
    "        self.tweets = Table('tweets', MetaData(), autoload=True, autoload_with=engine)\n",
    "        self.sentiments = Table('sentiments', MetaData(), autoload=True, autoload_with=engine)\n",
    "        \n",
    "        self.dataframe = self.read_data()\n",
    "        \n",
    "    def read_data(self):\n",
    "        stmt = (\n",
    "            select([\n",
    "                self.tweets.columns.id,\n",
    "                self.tweets.columns.body            \n",
    "            ])\n",
    "            .order_by(desc('id'))\n",
    "            .limit(10)\n",
    "        )\n",
    "        \n",
    "        return pd.DataFrame(self.engine.execute(stmt).fetchall())\n",
    "        \n",
    "    @staticmethod\n",
    "    def preprocess(text):\n",
    "            new_text = []\n",
    "            for t in text.split(\" \"):\n",
    "                t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "                t = 'http' if t.startswith('http') else t\n",
    "                new_text.append(t)\n",
    "            return \" \".join(new_text) \n",
    "    \n",
    "    def get_model_output(self, text):\n",
    "        encoded_input = self.tokenizer(text, return_tensors='pt')\n",
    "        output = self.model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        \n",
    "        return list(scores)\n",
    "    \n",
    "    def split_model_output(self):\n",
    "        model_output = np.array(list(self.dataframe['scores'].values))\n",
    "        \n",
    "        self.dataframe['positive'] = model_output[:, 2]\n",
    "        self.dataframe['negative'] = model_output[:, 0]\n",
    "        self.dataframe['neutral'] = model_output[:, 1]\n",
    "        \n",
    "        self.dataframe.drop(columns=['scores'], inplace=True)\n",
    "        \n",
    "    def update_id(self):\n",
    "        stmt = (\n",
    "            select(func.max(self.sentiments.columns.id))\n",
    "        )\n",
    "        new_id = self.engine.execute(stmt).fetchall()[0][0]\n",
    "        \n",
    "        if new_id is not None:\n",
    "            self.dataframe.index += new_id + 1\n",
    "            \n",
    "    def postprocessing(self):\n",
    "        self.dataframe.drop(columns=['body'], inplace=True)\n",
    "        \n",
    "        self.dataframe.rename(columns={'id': 'tweet_id'}, inplace=True)\n",
    "        \n",
    "        \n",
    "    def insert_result_into_database(self):\n",
    "        self.update_id()\n",
    "        \n",
    "        self.dataframe.to_sql('sentiments', self.engine, if_exists='append', index=True, index_label='id')\n",
    "        \n",
    "    def predict(self):\n",
    "        self.dataframe.body = self.dataframe.body.apply(Model.preprocess)\n",
    "        self.dataframe['scores'] = self.dataframe.body.apply(self.get_model_output)\n",
    "        \n",
    "        self.split_model_output()\n",
    "        \n",
    "        self.postprocessing()\n",
    "        \n",
    "        self.insert_result_into_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38b7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "db_string = \"postgresql://postgres:NaszPostgresikUkochany@34.32.246.110:5432/sandbox\"\n",
    "\n",
    "engine = create_engine(db_string)\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d75719ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3649</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>0.764061</td>\n",
       "      <td>0.226170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3648</td>\n",
       "      <td>0.334867</td>\n",
       "      <td>0.259942</td>\n",
       "      <td>0.405191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3647</td>\n",
       "      <td>0.789894</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.198451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3646</td>\n",
       "      <td>0.887917</td>\n",
       "      <td>0.014168</td>\n",
       "      <td>0.097915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3645</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.888397</td>\n",
       "      <td>0.101558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3644</td>\n",
       "      <td>0.135234</td>\n",
       "      <td>0.609589</td>\n",
       "      <td>0.255178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3643</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.476656</td>\n",
       "      <td>0.514507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3642</td>\n",
       "      <td>0.036169</td>\n",
       "      <td>0.785779</td>\n",
       "      <td>0.178052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3641</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.848672</td>\n",
       "      <td>0.138157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3640</td>\n",
       "      <td>0.163695</td>\n",
       "      <td>0.251535</td>\n",
       "      <td>0.584771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3649</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>0.764061</td>\n",
       "      <td>0.226170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>3648</td>\n",
       "      <td>0.334867</td>\n",
       "      <td>0.259942</td>\n",
       "      <td>0.405191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3647</td>\n",
       "      <td>0.789894</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.198451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3646</td>\n",
       "      <td>0.887917</td>\n",
       "      <td>0.014168</td>\n",
       "      <td>0.097915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3645</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.888397</td>\n",
       "      <td>0.101558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>3644</td>\n",
       "      <td>0.135234</td>\n",
       "      <td>0.609589</td>\n",
       "      <td>0.255178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>3643</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.476656</td>\n",
       "      <td>0.514507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>3642</td>\n",
       "      <td>0.036169</td>\n",
       "      <td>0.785779</td>\n",
       "      <td>0.178052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>3641</td>\n",
       "      <td>0.013170</td>\n",
       "      <td>0.848672</td>\n",
       "      <td>0.138157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>3640</td>\n",
       "      <td>0.163695</td>\n",
       "      <td>0.251535</td>\n",
       "      <td>0.584771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  tweet_id  positive  negative   neutral\n",
       "0    0      3649  0.009769  0.764061  0.226170\n",
       "1    1      3648  0.334867  0.259942  0.405191\n",
       "2    2      3647  0.789894  0.011655  0.198451\n",
       "3    3      3646  0.887917  0.014168  0.097915\n",
       "4    4      3645  0.010045  0.888397  0.101558\n",
       "5    5      3644  0.135234  0.609589  0.255178\n",
       "6    6      3643  0.008837  0.476656  0.514507\n",
       "7    7      3642  0.036169  0.785779  0.178052\n",
       "8    8      3641  0.013170  0.848672  0.138157\n",
       "9    9      3640  0.163695  0.251535  0.584771\n",
       "10  10      3649  0.009769  0.764061  0.226170\n",
       "11  11      3648  0.334867  0.259942  0.405191\n",
       "12  12      3647  0.789894  0.011655  0.198451\n",
       "13  13      3646  0.887917  0.014168  0.097915\n",
       "14  14      3645  0.010045  0.888397  0.101558\n",
       "15  15      3644  0.135234  0.609589  0.255178\n",
       "16  16      3643  0.008837  0.476656  0.514507\n",
       "17  17      3642  0.036169  0.785779  0.178052\n",
       "18  18      3641  0.013170  0.848672  0.138157\n",
       "19  19      3640  0.163695  0.251535  0.584771"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(engine.execute('SELECT * FROM sentiments').fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2e1e612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "Model(engine).predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
